{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    " \n",
    "class ActionResult(Enum):\n",
    "    CONTINUE = 0,\n",
    "    WIN = 1,\n",
    "    LOSE = 2\n",
    "        \n",
    "class FrozenLakeGame():\n",
    "          \n",
    "    def __init__(self, board):\n",
    "        self.board = board\n",
    "        self.num_actions = 4\n",
    "        self.num_states = board.size\n",
    "        self.nrows = board.shape[0]\n",
    "        self.ncols = board.shape[1]\n",
    "        self.action_dict = {\n",
    "            0 : 'R',\n",
    "            1 : 'D',\n",
    "            2 : 'L',\n",
    "            3 : 'U'\n",
    "        }\n",
    "        self.status_transitions = self._compute_action_map()\n",
    "        self.rewards = self._compute_reward_map()\n",
    "        self.current_state = 0\n",
    "        self.action_result = ActionResult.CONTINUE\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_state = 0\n",
    "        self.action_result = ActionResult.CONTINUE\n",
    "        \n",
    "    def step(self, action_id):\n",
    "        \n",
    "        if self.action_result is not ActionResult.CONTINUE:\n",
    "            return self.current_state, self.current_result, 0\n",
    "            \n",
    "        new_state = self.status_transitions[self.current_state, action_id]\n",
    "        reward = self.rewards[self.current_state, new_state]        \n",
    "        \n",
    "        if self._get_state_value(new_state) == 'I':\n",
    "            self.action_result = ActionResult.CONTINUE\n",
    "            \n",
    "            self.current_state = new_state\n",
    "            \n",
    "        if self._get_state_value(new_state) == 'H':\n",
    "            self.action_result = ActionResult.LOSE\n",
    "            self.current_state = new_state\n",
    "        \n",
    "        if self._get_state_value(new_state) == 'G':\n",
    "            self.action_result = ActionResult.WIN\n",
    "            self.current_state = new_state\n",
    "        \n",
    "        return self.current_state, self.action_result, reward\n",
    "        \n",
    "    def _compute_reward_map(self):\n",
    "        rewards = np.zeros((self.num_states, self.num_states))\n",
    "        rewards[:,self.num_states-1] = 1\n",
    "        return rewards\n",
    "    \n",
    "    def _compute_action_map(self):\n",
    "        status_transitions = np.zeros((self.num_states, self.num_actions),'uint8')\n",
    "        for i in range(self.nrows):\n",
    "            for j in range(self.ncols):\n",
    "                s = i*self.ncols + j\n",
    "                status_transitions[s,0] = (i*self.ncols + j+1) if (j+1) < self.ncols else s\n",
    "                status_transitions[s,1]= ((i+1)*self.ncols + j) if (i+1) < self.nrows else s\n",
    "                status_transitions[s,2]= (i*self.ncols + j-1) if (j-1) >= 0 else s\n",
    "                status_transitions[s,3]= ((i-1)*self.ncols + j) if (i-1) >= 0 else s\n",
    "        return status_transitions\n",
    "                \n",
    "    def translate_action(self, action_id):\n",
    "        return self.action_dict[action_id]\n",
    "    \n",
    "    def print_board(self):\n",
    "        print(pd.DataFrame(self.board).to_string(header=False, index=False))\n",
    "        \n",
    "    def print_transitions(self):\n",
    "        print(pd.DataFrame(self.status_transitions).to_string(header=['R','D','L','U']))\n",
    "    \n",
    "    def print_reward_map(self):\n",
    "        print(pd.DataFrame(self.rewards).to_string())\n",
    "            \n",
    "    def _get_state_value(self, state):\n",
    "        i,j = np.unravel_index(state, self.board.shape)\n",
    "        return self.board[i,j]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = np.array((\n",
    "    ('S','I','I','I'),\n",
    "    ('I','H','I','H'),\n",
    "    ('I','H','I','H'),\n",
    "    ('H','I','I','G')\n",
    "))\n",
    "\n",
    "game = FrozenLakeGame(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Board:')\n",
    "game.print_board()\n",
    "print('\\nRewards:')\n",
    "game.print_reward_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rate = 0.9\n",
    "learning_rate = 0.1\n",
    "learning_rate_decay = 0.0\n",
    "episodes = 10000\n",
    "e = 0.4\n",
    "e_decay = 0.0\n",
    "\n",
    "Q_list = [] # It will contain the evolution of the Q-matrix\n",
    "reward_list = []\n",
    "Q = np.random.rand(game.num_states,game.num_actions) # Initialized randomly\n",
    "\n",
    "def choose_best_action(q_status):\n",
    "    return np.argmax(q_status)\n",
    "\n",
    "def e_greedy_strategy(e, Q, current_state):\n",
    "    if np.random.rand(1) < e:\n",
    "        action = np.random.randint(Q.shape[1])\n",
    "    else:\n",
    "        action = choose_best_action(Q[current_state,:])\n",
    "    return action\n",
    "\n",
    "for n in range(episodes):\n",
    "    current_state = 0\n",
    "    game.reset()\n",
    "    while 1:\n",
    "        # Choose and perform next action\n",
    "        action = e_greedy_strategy(e, Q, current_state)\n",
    "        new_state, action_result, reward = game.step(action)\n",
    "        \n",
    "        if action_result == ActionResult.LOSE:\n",
    "            break\n",
    "        \n",
    "        # Update Q matrix\n",
    "        best_action = choose_best_action(Q[new_state,:])\n",
    "        Q[current_state,action] = (1.0 - learning_rate) * Q[current_state,action] + learning_rate * (reward + discount_rate * Q[new_state,best_action])\n",
    "        \n",
    "        current_state = new_state\n",
    "\n",
    "        if action_result == ActionResult.WIN:\n",
    "            break\n",
    "\n",
    "    Q_list.append(np.copy(Q))\n",
    "    reward_list.append(reward)\n",
    "    \n",
    "    # update rates\n",
    "    learning_rate *= (1-learning_rate_decay)\n",
    "    e *= (1-e_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q=Q_list[-1]\n",
    "print(Q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_actions = np.reshape(np.argmax(Q,1), game.board.shape)\n",
    "\n",
    "movements = []\n",
    "s = 0\n",
    "\n",
    "game.reset()\n",
    "result = ActionResult.CONTINUE\n",
    "while result == ActionResult.CONTINUE:\n",
    "    i,j = np.unravel_index(s, board.shape)\n",
    "    \n",
    "    action = optimum_actions[i,j]\n",
    "    movements.append(game.translate_action(action))\n",
    "    s, result, reward = game.step(action)\n",
    "\n",
    "    if len(movements) > 20:\n",
    "        print('In a looooop!!')\n",
    "        break\n",
    "\n",
    "print(result)\n",
    "print(movements)\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
